---
title: "XPATH and Data Extraction"
subtitle: "Using rvest"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(httr)
library(rvest)
library(magrittr)
```

## Key Functions: `html_nodes`

- `html_nodes(x, "path")` extracts all elements from the page `x` that have the tag / class / id `path`. (Use SelectorGadget to determine `path`.) 
- `html_node()` does the same thing but only returns the first matching element. 
- Can be chained

```{r nodesex}
myurl <- "https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(PPP)_per_capita"
myhtml <- read_html(myurl)
myhtml %>% 
 html_nodes("table") %>%
 magrittr::extract2(2) %>%
 html_table(header = TRUE) %>% 
 head

myhtml %>% 
  html_nodes("p") %>% # first get all the paragraphs 
  html_nodes("a") # then get all the links in those paragraphs
```

---

## Key Functions: `html_text`

- `html_text(x)` extracts all text from the nodeset `x` 
- Good for cleaning output

```{r textex}
myhtml %>% 
  html_nodes("p") %>% # first get all the paragraphs 
  html_nodes("a") %>% # then get all the links in those paragraphs
  html_text() # get the linked text only 
```

---

## Key Functions: `html_table` 

- `html_table(x, header, fill)` - parse html table(s) from `x` into a data frame or list of data frames 
- Structure of HTML makes finding and extracting tables easy!

```{r tableex}
myhtml %>% 
  html_nodes("table") %>% # get the tables 
  head(2) # look at first 2
```

```{r tableex2}
myhtml %>% 
  html_nodes("table") %>% # get the tables 
  magrittr::extract2(2) %>% # pick the second one to parse
  html_table(header = TRUE) # parse table 
```

---

## Key functions: `html_attrs`

- `html_attrs(x)` - extracts all attribute elements from a nodeset `x`
- `html_attr(x, name)` - extracts the `name` attribute from all elements in nodeset `x`
- Attributes are things in the HTML like `href`, `title`, `class`, `style`, etc.
- Use these functions to find and extract your data

```{r attrsex}
myhtml %>% 
  html_nodes("table") %>% 
  magrittr::extract2(2) %>%
  html_attrs()
```

```{r attrsex2}
myhtml %>% 
  html_nodes("p") %>% 
  html_nodes("a") %>%
  html_attr("href")
```

---

## Other functions

- `html_children` - list the "children" of the HTML page. Can be chained like `html_nodes`
- `html_name` - gives the tags of a nodeset. Use in a chain with `html_children`

```{r childex}
myhtml %>% 
  html_children() %>% 
  html_name()
```

- `html_form` - parses HTML forms (checkboxes, fill-in-the-blanks, etc.)
- `html_session` - simulate a session in an html browser; use the functions `jump_to`, `back` to navigate through the page

---

## Your Turn #2

Find another website you want to scrape (ideas: [all bills in the house so far this year](https://www.congress.gov/search), [video game reviews](http://www.metacritic.com/game), anything Wikipedia) and use *at least* 3 different `rvest` functions in a chain to extract some data.

---

## Advanced Example: Inaugural Addresses
**The Data**
- [The Avalon Project](http://avalon.law.yale.edu/subject_menus/inaug.asp) has most of the U.S. Presidential inaugural addresses. 
- Obama & Trump's ('13, '17), VanBuren 1837, Buchanan 1857, Garfield 1881, and Coolidge 1925 are missing, but are easily found elsewhere. I have them saved as text files on the website. 
- Let's scrape all of them from The Avalon Project! 

---

## Get data frame of addresses

- Could use another source to get this data of President names and years of inaugurations, but we'll use The Avalon Project's site because it's a good example of data that needs tidying. 
```{r getyears}
url <- "http://avalon.law.yale.edu/subject_menus/inaug.asp"
# even though it's called "all inaugs" some are missing
all_inaugs <- (url %>% 
  read_html() %>% 
  html_nodes("table") %>% 
  html_table(fill=T, header = T)) %>% extract2(3)
# tidy table of addresses
all_inaugs_tidy <- all_inaugs %>% 
  gather(term, year, -President) %>% 
  filter(!is.na(year)) %>% 
  select(-term) %>% 
  arrange(year)
head(all_inaugs_tidy)
```

---

## Get links to visit & scrape

```{r getlinks}
# get the links to the addresses 
inaugadds_adds <- (url %>%
  read_html() %>%
  html_nodes("a") %>%
  html_attr("href"))[12:66]
# create the urls to scrape
urlstump <- "http://avalon.law.yale.edu/"
inaugurls <- paste0(urlstump, str_replace(inaugadds_adds, "../", ""))
all_inaugs_tidy$url <- inaugurls
head(all_inaugs_tidy)
```

---

## Automate scraping

- A function to read the addresses and get the text of the speeches, with a catch for a read error
```{r functiongetspeech, cache=TRUE, message = FALSE, warning = FALSE}
get_inaugurations <- function(url){
  test <- try(url %>% read_html(), silent=T)
  if ("try-error" %in% class(test)) {
    return(NA)
  } else
    url %>% read_html() %>%
      html_nodes("p") %>% 
      html_text() -> address
    return(unlist(address))
}

# takes about 30 secs to run
all_inaugs_text <- all_inaugs_tidy %>% 
  mutate(address_text = (map(url, get_inaugurations))) 

all_inaugs_text$address_text[[1]]
```

---

## Add Missings

```{r missings}
all_inaugs_text$President[is.na(all_inaugs_text$address_text)]
# there are 7 missing at this point: obama's and trump's, plus coolidge, garfield, buchanan, and van buren, which errored in the scraping.
obama09 <- get_inaugurations("http://avalon.law.yale.edu/21st_century/obama.asp")
obama13 <- readLines("speeches/obama2013.txt")
trump17 <- readLines("speeches/trumpinaug.txt")
vanburen1837 <- readLines("speeches/vanburen1837.txt") # row 13
buchanan1857 <- readLines("speeches/buchanan1857.txt") # row 18
garfield1881 <- readLines("speeches/garfield1881.txt") # row 24
coolidge1925 <- readLines("speeches/coolidge1925.txt") # row 35
all_inaugs_text$address_text[c(13,18,24,35)] <- list(vanburen1837,buchanan1857, garfield1881, coolidge1925)

# lets combine them all now
recents <- data.frame(President = c(rep("Barack Obama", 2), 
                                    "Donald Trump"),
                      year = c(2009, 2013, 2017), 
                      url = NA,
                      address_text = NA)

all_inaugs_text <- rbind(all_inaugs_text, recents)
all_inaugs_text$address_text[c(56:58)] <- list(obama09, obama13, trump17)
```

---

## Check-in: What did we do?

1. We found some interesting data to scrape from the web using `rvest`.
2. We used `tidyr` to create tidy data: A data frame of President and year. One observation per row!
3. We used the consistent HTML structure of the urls we wanted to scrape to automate collection of web data
    + Way faster than copy-paste! 
    + Though  we had to do some by hand, we took advantage of the tidy data and added the missing data manually without much pain.
4. We now have a tidy data set of Presidential inaugural addresses for text analysis!

--- 

## A (Small) Text Analysis

Now, I use the [`tidytext`](http://tidytextmining.com/) package to get the words out of each inaugural address. 

```{r textanalysis}
# install.packages("tidytext")
library(tidytext)
all_inaugs_text %>% 
  select(-url) %>% 
  unnest() %>% 
  unnest_tokens(word, address_text) -> presidential_words
head(presidential_words)
```

---

## Longest speeches

```{r longestspeech}
presidential_words %>% 
  group_by(President,year) %>% 
  summarize(num_words = n()) %>%
  arrange(desc(num_words)) -> presidential_wordtotals
```

```{r speechplot, echo = FALSE, fig.align='center', cache = T, fig.height=5, fig.width=9.5}
library(ggrepel)
ggplot(presidential_wordtotals) + 
  geom_bar(aes(x = reorder(year, num_words), y = num_words), stat = "identity", fill = 'white', color = 'black') + 
  geom_label_repel(aes(x = reorder(year, num_words), y = num_words, label = President), size = 2.5) +
  labs(y = "Word count of Speech", x = "Year (sorted by word count)", title = "Length of Presidential Inaugural Addresses", subtitle = "Max: 8,459; Min: 135; Median: 2,090; Mean: 2,341") + 
  theme(axis.text.x = element_text(angle = 45, size = 7), plot.subtitle = element_text(hjust = .5))
```
